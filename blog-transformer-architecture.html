<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"><title>Transformer Architecture - AI/ML Blogs</title><link rel="stylesheet" href="style.css"></head><body><div class="container"><aside class="sidebar"><div class="sidebar-header"><h1>AI/ML Blogs</h1><p class="subtitle">Exploring the Future of Technology</p></div><div class="search-box"><input type="text" id="searchVlogs" placeholder="Search blogs..."></div><div class="vlogs-list" id="blogsList"><a class="vlog-item" href="blog-how-llms-generate-text.html"><div class="vlog-number">01</div><div class="vlog-info"><h3>How LLMs Generate Text</h3><span class="vlog-date">Nov 16, 2025</span></div></a><a class="vlog-item" href="blog-deep-learning-fundamentals.html"><div class="vlog-number">02</div><div class="vlog-info"><h3>Deep Learning Fundamentals</h3><span class="vlog-date">Nov 12, 2025</span></div></a><a class="vlog-item" href="blog-computer-vision-cnns.html"><div class="vlog-number">03</div><div class="vlog-info"><h3>Computer Vision & CNNs</h3><span class="vlog-date">Nov 14, 2025</span></div></a><a class="vlog-item" href="blog-natural-language-processing.html"><div class="vlog-number">04</div><div class="vlog-info"><h3>Natural Language Processing</h3><span class="vlog-date">Nov 15, 2025</span></div></a><a class="vlog-item active" href="blog-transformer-architecture.html"><div class="vlog-number">05</div><div class="vlog-info"><h3>Transformer Architecture</h3><span class="vlog-date">Nov 16, 2025</span></div></a><a class="vlog-item" href="blog-reinforcement-learning-basics.html"><div class="vlog-number">06</div><div class="vlog-info"><h3>Reinforcement Learning Basics</h3><span class="vlog-date">Coming Soon</span></div></a></div></aside><main class="main-content"><div class="content-header"><div class="breadcrumb"><a href="index.html" style="color:var(--text-secondary);text-decoration:none;">Home</a> / Blogs / <span class="current-vlog">Transformer Architecture</span></div></div><article class="vlog-content"><div class="vlog-header"><div class="vlog-meta"><span class="badge">Transformers</span><span class="reading-time">11 min read</span></div><h1 class="vlog-title">Transformer Architecture</h1><p class="vlog-subtitle">Attention mechanisms reshaping AI capability.</p><div class="author-info"><div class="author-avatar">HG</div><div class="author-details"><p class="author-name">Harshad Gore</p><p class="publish-date">Published Nov 16, 2025</p></div></div></div><div class="vlog-body"><section class="hero-image"><img src="llm.png" alt="Transformer architecture" /></section><section class="content-section"><h2>Core Ideas</h2><ul class="custom-list"><li><strong>Self-Attention:</strong> Tokens attend to each other for contextual weighting.</li><li><strong>Multi-Head:</strong> Parallel attention subspaces.</li><li><strong>Positional Encoding:</strong> Inject order into otherwise permutation-invariant attention.</li><li><strong>Feed-Forward:</strong> Per-token transformation after attention mixing.</li></ul></section><section class="content-section"><h2>Scaled Dot-Product Attention (PyTorch)</h2><div class="code-block"><pre><code>import torch
import torch.nn.functional as F

def attention(q, k, v):
    scale = q.size(-1) ** 0.5
    scores = (q @ k.transpose(-2, -1)) / scale
    weights = F.softmax(scores, dim=-1)
    return weights @ v</code></pre></div></section></div><div class="vlog-footer"><div class="tags"><span class="tag">#Transformers</span><span class="tag">#Attention</span><span class="tag">#AI</span></div><div class="navigation-buttons"><button class="nav-btn" onclick="location.href='blog-natural-language-processing.html'">← Previous</button><button class="nav-btn" onclick="location.href='blog-reinforcement-learning-basics.html'">Next →</button></div></div></article></main></div><script src="script.js"></script></body></html>